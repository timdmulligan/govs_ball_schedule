{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Optimal Gov's Ball Scedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bring in the neccessary libraries \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "import sqlite3 \n",
    "import time\n",
    "import numpy \n",
    "import signal\n",
    "from urllib.request import Request, urlopen\n",
    "from IPython.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress Bar Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load package for progress bar\n",
    "%run \"progress_bar.py\"\n",
    "print ('Progress Bar Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Gov's Ball Artist Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab the artist info from the Gov's ball website\n",
    "with urllib.request.urlopen(\"https://www.governorsballmusicfestival.com/lineup/interactive-lineup/\") as url:\n",
    "    s = url.read()\n",
    "soup = BeautifulSoup(s, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = soup.findAll('div', {\"class\":\"c-lineup__artist\"})\n",
    "#Put those names in a list, then a dataframe\n",
    "govs_ball_data = pd.DataFrame([])\n",
    "for i in info:\n",
    "    artist = i.attrs['data-title']\n",
    "    date = i.attrs[\"data-day-titles\"]\n",
    "    govs_ball_data = govs_ball_data.append(pd.DataFrame({'artist': artist, #Create the table\n",
    "                                                         'date'  : date},\n",
    "                                                  index=[0]),\n",
    "                                     ignore_index=True)\n",
    "govs_ball_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "govs_ball_data[\"day\"] = ((govs_ball_data[\"date\"].str.slice(-5, -4)).astype(int) -1).astype(str)\n",
    "govs_ball_data[\"date\"] = govs_ball_data[\"date\"].str.slice(2, -2)\n",
    "govs_ball_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Names to a SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"pitchfork-data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "govs_ball_data.to_sql(\"govs_ball_data\", con,if_exists='replace')\n",
    "con.close() #close db connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pitchfork Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The crawling happens in two distinct stages. In Stage 1, the code loops throught the reviews page on pitchfork.com to find links to all the reviews. Stage 2 goes to each link and pull various bits of information. There's lots more to pull, but this is a solid starting place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249\n"
     ]
    }
   ],
   "source": [
    "#Stage 1\n",
    "con = sqlite3.connect(\"pitchfork-data.db\") #connect to db\n",
    "\n",
    "for i in log_progress(range(1198,1250), every=1): \n",
    "    #Use the range function to decide how many pages you want to go through\n",
    "    #In this case, I'm going through the latest 15,000 reviews (12 per page)\n",
    "    page_no = str(i)\n",
    "    link = ('http://pitchfork.com/reviews/albums/?page=' + page_no) #create the link\n",
    "    t0 = time.time()\n",
    "    req = Request(link, headers={ 'User-Agent': 'Firefox/24.0' })\n",
    "    webpage = urlopen(req).read()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(10*response_delay)  # wait 10x longer than it took them to respond\n",
    "    soup = BeautifulSoup(webpage, \"lxml\") #create the soup\n",
    "    info = soup.findAll('a', {\"class\":\"album-link\"}) #pull the album link\n",
    "    for j in info:\n",
    "            pd.DataFrame({'link': j.attrs['href']}, #Create the table\n",
    "                         index=[0]).to_sql(\"link_table\",\n",
    "                                           con,\n",
    "                                           if_exists = \"append\")\n",
    "    clear_output() #clear ouput before rewriting progress\n",
    "    print (i)\n",
    "\n",
    "con.close() #close db connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Mix and the Remix ,  Wall of Noise 1.167588233947754 90\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'http://www.pitchfork.com'\n",
    "con = sqlite3.connect(\"pitchfork-data.db\")\n",
    "links_table = pd.read_sql_query(\"SELECT * from link_table\", con)\n",
    "links = links_table[\"link\"]\n",
    "iterator = 0\n",
    "\n",
    "class Timeout(Exception):  # handles timeout errors (e.g., server request is taking too long)\n",
    "    pass\n",
    "\n",
    "for i in log_progress(links[14921:], every=1):\n",
    "    link = BASE_URL + i\n",
    "    t0 = time.time()\n",
    "    req = Request(link, headers={ 'User-Agent': 'Firefox/24.0' })\n",
    "    webpage = urlopen(req).read()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(4*response_delay)  # wait Nx longer than it took them to respond\n",
    "    soup = BeautifulSoup(webpage, \"lxml\") #same as above\n",
    "    artist_info = soup.findAll('ul', {\"class\":\"artist-links artist-list\"}) #Artist Name\n",
    "    album_info = soup.findAll('h1', {\"class\":\"review-title\"}) #Album Name\n",
    "    score_info = soup.findAll('div', {\"class\":\"score-circle\"}) #Score\n",
    "    pub_info = soup.findAll('span', {\"class\":\"pub-date\"}) # Publication Date\n",
    "    genre_info = soup.findAll('ul', {\"class\":\"genre-list before\"}) #Genre\n",
    "    for j in artist_info:\n",
    "        artist = j.text\n",
    "    for k in album_info:\n",
    "        album = k.text\n",
    "    for l in score_info:\n",
    "        score = l.text\n",
    "    for m in pub_info:\n",
    "        pub_date = m.text\n",
    "    for n in genre_info:\n",
    "        genre = n.text\n",
    "    clear_output()\n",
    "    print (artist, \", \", album, response_delay, iterator)\n",
    "    pd.DataFrame({'artist': artist, #Create the table\n",
    "                  'album'  : album,\n",
    "                  'score' : score,\n",
    "                  'pub_date' : pub_date,\n",
    "                  'genre' : genre,\n",
    "                  'link' : link},\n",
    "                 index=[0]).to_sql(\"album_table\",\n",
    "                                   con,\n",
    "                                   if_exists = \"append\")\n",
    "    iterator= iterator +1\n",
    "\n",
    "con.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"pitchfork-data.db\")\n",
    "#Pull the table we just wrote back it\n",
    "album_table = pd.read_sql_query(\"SELECT * from album_table\", con)\n",
    "#Drop any duplicates that may have happened\n",
    "album_table = album_table.drop_duplicates()\n",
    "#Delete the index\n",
    "del album_table[\"index\"]\n",
    "#reupload as album_table_clean\n",
    "album_table.to_sql(\"album_table_clean\", con, if_exists = \"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "album_table = pd.read_sql_query(\"SELECT * from album_table_clean\", con)\n",
    "album_table[\"artist\"] = album_table[\"artist\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artists = album_table[\"artist\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 4)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_data = pd.read_sql_query(\"SELECT * from govs_ball_data\", con)\n",
    "gb_data[\"artist\"] = gb_data[\"artist\"].str.upper()\n",
    "gb_data.append([{\"index\":\"11\",\n",
    "                 \"artist\":\"KEVIN PARKER\",\n",
    "                 \"date\":\"Saturday, June 3rd\",\n",
    "                 \"day\":\"2\"}])\n",
    "gb_data[~gb_data.artist.isin(artists)].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_data[\"artist\"] = np.where(gb_data[\"artist\"] == \"CHARLES BRADLEY & HIS EXTRAORDINAIRES\",\n",
    "                            \"CHARLES BRADLEY\",\n",
    "                            gb_data[\"artist\"])\n",
    "gb_data[\"artist\"] = np.where(gb_data[\"artist\"] == \"MARK RONSON VS KEVIN PARKER\",\n",
    "                            \"MARK RONSON\",\n",
    "                            gb_data[\"artist\"])\n",
    "\n",
    "gb_data[~gb_data.artist.isin(artists)].shape\n",
    "#x number of artists missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotify API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feed the Gov's Ball Artists into Spotify\n",
    "spotify = spotipy.Spotify()#Create spotify object from spotipy \n",
    "\n",
    "count = 0\n",
    "spotify_data = pd.DataFrame([])\n",
    "for name in log_progress(gb_data[\"artist\"], every = 1):\n",
    "    search = spotify.search(q='artist:' + name, type='artist')\n",
    "    #Search and grab the first result\n",
    "    name = pd.read_json(json.dumps(search[\"artists\"]))[\"items\"][0][\"name\"]\n",
    "    popularity = pd.read_json(json.dumps(search[\"artists\"]))[\"items\"][0][\"popularity\"]\n",
    "    genre = pd.read_json(json.dumps(search[\"artists\"]))[\"items\"][0][\"genres\"]\n",
    "    followers = pd.read_json(json.dumps(search[\"artists\"]))[\"items\"][0][\"followers\"][\"total\"]\n",
    "    if len(genre) > 0:\n",
    "        genre = \"/\".join(genre)\n",
    "    else:\n",
    "        genre = \"Unknown\"\n",
    "    spotify_data = spotify_data.append(pd.DataFrame({'artist': name,\n",
    "                                                     'popularity': popularity,\n",
    "                                                     'followers' : followers,\n",
    "                                                     'genre' : genre},\n",
    "                                                    index=[count]))\n",
    "    count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "87fc581e8a5b4f17929a1739697a7f5a": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
