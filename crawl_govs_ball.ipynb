{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Optimal Gov's Ball Scedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bring in the neccessary libraries \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "import sqlite3 \n",
    "import time\n",
    "import numpy \n",
    "import signal\n",
    "from urllib.request import Request, urlopen\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Gov's Ball Artist Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab the artist info from the Gov's ball website\n",
    "with urllib.request.urlopen(\"https://www.governorsballmusicfestival.com/lineup/interactive-lineup/\") as url:\n",
    "    s = url.read()\n",
    "soup = BeautifulSoup(s, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = soup.findAll('div', {\"class\":\"c-lineup__artist\"})\n",
    "#Put those names in a list, then a dataframe\n",
    "govs_ball_data = pd.DataFrame([])\n",
    "for i in info:\n",
    "    artist = i.attrs['data-title']\n",
    "    date = i.attrs[\"data-day-titles\"]\n",
    "    govs_ball_data = govs_ball_data.append(pd.DataFrame({'artist': artist, #Create the table\n",
    "                                                         'date'  : date},\n",
    "                                                  index=[0]),\n",
    "                                     ignore_index=True)\n",
    "govs_ball_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "govs_ball_data[\"day\"] = ((govs_ball_data[\"date\"].str.slice(-5, -4)).astype(int) -1).astype(str)\n",
    "govs_ball_data[\"date\"] = govs_ball_data[\"date\"].str.slice(2, -2)\n",
    "govs_ball_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Names to a SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"pitchfork-data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "govs_ball_data.to_sql(\"govs_ball_data\", con,if_exists='replace')\n",
    "con.close() #close db connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pitchfork Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The crawling happens in two distinct stages. In Stage 1, the code loops throught the reviews page on pitchfork.com to find links to all the reviews. Stage 2 goes to each link and pull various bits of information. There's lots more to pull, but this is a solid starting place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "#Stage 1\n",
    "con = sqlite3.connect(\"pitchfork-data.db\") #connect to db\n",
    "\n",
    "for i in range(1,1500): #Use the range function to decide how many pages you want to go through \n",
    "    page_no = str(i)\n",
    "    link = ('http://pitchfork.com/reviews/albums/?page=' + page_no) #create the link\n",
    "    t0 = time.time()\n",
    "    req = Request(link, headers={ 'User-Agent': 'Firefox/24.0' })\n",
    "    webpage = urlopen(req).read()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(5*response_delay)  # wait 10x longer than it took them to respond\n",
    "    soup = BeautifulSoup(webpage, \"lxml\") #create the soup\n",
    "    info = soup.findAll('a', {\"class\":\"album-link\"}) #pull the album link\n",
    "    for j in info:\n",
    "            pd.DataFrame({'link': j.attrs['href']}, #Create the table\n",
    "                         index=[0]).to_sql(\"link_table\",\n",
    "                                           con,\n",
    "                                           if_exists = \"append\")\n",
    "    clear_output() #clear ouput before rewriting progress\n",
    "    print (i)\n",
    "\n",
    "con.close() #close db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_table = pd.DataFrame([])\n",
    "link_table[\"links\"] = links\n",
    "link_table[:3000].to_sql(\"link_table_first_3000\", con)\n",
    "link_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "album_table = pd.DataFrame([]) #Create an empty dataframe that'll hold the info for each album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.pitchfork.com'\n",
    "\n",
    "for i in links[2552:]:\n",
    "    link = BASE_URL + i\n",
    "    t0 = time.time()\n",
    "    req = Request(link, headers={ 'User-Agent': 'Firefox/24.0' })\n",
    "    webpage = urlopen(req).read()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(4*response_delay)  # wait 10x longer than it took them to respond\n",
    "    soup = BeautifulSoup(webpage, \"lxml\") #same as above\n",
    "    artist_info = soup.findAll('ul', {\"class\":\"artist-links artist-list\"}) #Artist Name\n",
    "    album_info = soup.findAll('h1', {\"class\":\"review-title\"}) #Album Name\n",
    "    score_info = soup.findAll('div', {\"class\":\"score-circle\"}) #Score\n",
    "    pub_info = soup.findAll('span', {\"class\":\"pub-date\"}) # Publication Date\n",
    "    genre_info = soup.findAll('ul', {\"class\":\"genre-list before\"}) #Genre\n",
    "    for j in artist_info:\n",
    "        artist = j.text\n",
    "    for k in album_info:\n",
    "        album = k.text\n",
    "    for l in score_info:\n",
    "        score = l.text\n",
    "    for m in pub_info:\n",
    "        pub_date = m.text\n",
    "    for n in genre_info:\n",
    "        genre = n.text\n",
    "    print (artist, \", \", album)\n",
    "    pd.DataFrame({'artist': artist, #Create the table\n",
    "                  'album'  : album,\n",
    "                  'score' : score,\n",
    "                  'pub_date' : pub_date,\n",
    "                  'genre' : genre},\n",
    "                 index=[0]).to_sql(\"album_table\",\n",
    "                                   con,\n",
    "                                   if_exists = \"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"pitchfork-data.db\")\n",
    "#Pull the table we just wrote back it\n",
    "album_table = pd.read_sql_query(\"SELECT * from album_table\", con)\n",
    "#Drop any duplicates that may have happened\n",
    "album_table = album_table.drop_duplicates()\n",
    "#Delete the index\n",
    "del album_table[\"index\"]\n",
    "#reupload as album_table_clean\n",
    "album_table.to_sql(\"album_table_clean\", con, if_exists = \"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
