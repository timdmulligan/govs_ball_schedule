{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Optimal Gov's Ball Scedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bring in the neccessary libraries \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "import sqlite3 \n",
    "import time\n",
    "import numpy \n",
    "import signal\n",
    "from urllib.request import Request, urlopen\n",
    "from IPython.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress Bar Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load package for progress bar\n",
    "%run \"progress_bar.py\"\n",
    "print ('Progress Bar Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Gov's Ball Artist Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grab the artist info from the Gov's ball website\n",
    "with urllib.request.urlopen(\"https://www.governorsballmusicfestival.com/lineup/interactive-lineup/\") as url:\n",
    "    s = url.read()\n",
    "soup = BeautifulSoup(s, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = soup.findAll('div', {\"class\":\"c-lineup__artist\"})\n",
    "#Put those names in a list, then a dataframe\n",
    "govs_ball_data = pd.DataFrame([])\n",
    "for i in info:\n",
    "    artist = i.attrs['data-title']\n",
    "    date = i.attrs[\"data-day-titles\"]\n",
    "    govs_ball_data = govs_ball_data.append(pd.DataFrame({'artist': artist, #Create the table\n",
    "                                                         'date'  : date},\n",
    "                                                  index=[0]),\n",
    "                                     ignore_index=True)\n",
    "govs_ball_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "govs_ball_data[\"day\"] = ((govs_ball_data[\"date\"].str.slice(-5, -4)).astype(int) -1).astype(str)\n",
    "govs_ball_data[\"date\"] = govs_ball_data[\"date\"].str.slice(2, -2)\n",
    "govs_ball_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Names to a SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"pitchfork-data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "govs_ball_data.to_sql(\"govs_ball_data\", con,if_exists='replace')\n",
    "con.close() #close db connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in log_progress(range(1,10), every=1): #sample progress bar\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pitchfork Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The crawling happens in two distinct stages. In Stage 1, the code loops throught the reviews page on pitchfork.com to find links to all the reviews. Stage 2 goes to each link and pull various bits of information. There's lots more to pull, but this is a solid starting place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249\n"
     ]
    }
   ],
   "source": [
    "#Stage 1\n",
    "con = sqlite3.connect(\"pitchfork-data.db\") #connect to db\n",
    "\n",
    "for i in log_progress(range(1198,1250), every=1): \n",
    "    #Use the range function to decide how many pages you want to go through\n",
    "    #In this case, I'm going through the latest 15,000 reviews (12 per page)\n",
    "    page_no = str(i)\n",
    "    link = ('http://pitchfork.com/reviews/albums/?page=' + page_no) #create the link\n",
    "    t0 = time.time()\n",
    "    req = Request(link, headers={ 'User-Agent': 'Firefox/24.0' })\n",
    "    webpage = urlopen(req).read()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(10*response_delay)  # wait 10x longer than it took them to respond\n",
    "    soup = BeautifulSoup(webpage, \"lxml\") #create the soup\n",
    "    info = soup.findAll('a', {\"class\":\"album-link\"}) #pull the album link\n",
    "    for j in info:\n",
    "            pd.DataFrame({'link': j.attrs['href']}, #Create the table\n",
    "                         index=[0]).to_sql(\"link_table\",\n",
    "                                           con,\n",
    "                                           if_exists = \"append\")\n",
    "    clear_output() #clear ouput before rewriting progress\n",
    "    print (i)\n",
    "\n",
    "con.close() #close db connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.pitchfork.com'\n",
    "con = sqlite3.connect(\"pitchfork-data.db\")\n",
    "links_table = pd.read_sql_query(\"SELECT * from link_table\", con)\n",
    "links = links_table[\"link\"]\n",
    "iterator = 0\n",
    "\n",
    "class Timeout(Exception):  # handles timeout errors (e.g., server request is taking too long)\n",
    "    pass\n",
    "\n",
    "for i in log_progress(links[7467:], every=1):\n",
    "    link = BASE_URL + i\n",
    "    t0 = time.time()\n",
    "    req = Request(link, headers={ 'User-Agent': 'Firefox/24.0' })\n",
    "    webpage = urlopen(req).read()\n",
    "    response_delay = time.time() - t0\n",
    "    time.sleep(8*response_delay)  # wait 10x longer than it took them to respond\n",
    "    soup = BeautifulSoup(webpage, \"lxml\") #same as above\n",
    "    artist_info = soup.findAll('ul', {\"class\":\"artist-links artist-list\"}) #Artist Name\n",
    "    album_info = soup.findAll('h1', {\"class\":\"review-title\"}) #Album Name\n",
    "    score_info = soup.findAll('div', {\"class\":\"score-circle\"}) #Score\n",
    "    pub_info = soup.findAll('span', {\"class\":\"pub-date\"}) # Publication Date\n",
    "    genre_info = soup.findAll('ul', {\"class\":\"genre-list before\"}) #Genre\n",
    "    for j in artist_info:\n",
    "        artist = j.text\n",
    "    for k in album_info:\n",
    "        album = k.text\n",
    "    for l in score_info:\n",
    "        score = l.text\n",
    "    for m in pub_info:\n",
    "        pub_date = m.text\n",
    "    for n in genre_info:\n",
    "        genre = n.text\n",
    "    clear_output()\n",
    "    print (artist, \", \", album, response_delay, iterator)\n",
    "    pd.DataFrame({'artist': artist, #Create the table\n",
    "                  'album'  : album,\n",
    "                  'score' : score,\n",
    "                  'pub_date' : pub_date,\n",
    "                  'genre' : genre,\n",
    "                  'link' : link},\n",
    "                 index=[0]).to_sql(\"album_table\",\n",
    "                                   con,\n",
    "                                   if_exists = \"append\")\n",
    "    iterator= iterator +1\n",
    "\n",
    "con.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"pitchfork-data.db\")\n",
    "#Pull the table we just wrote back it\n",
    "album_table = pd.read_sql_query(\"SELECT * from album_table\", con)\n",
    "#Drop any duplicates that may have happened\n",
    "album_table = album_table.drop_duplicates()\n",
    "#Delete the index\n",
    "del album_table[\"index\"]\n",
    "#reupload as album_table_clean\n",
    "album_table.to_sql(\"album_table_clean\", con, if_exists = \"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "02a7db5d43a342c2a96985ca2e625790": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
